# Distributed-Computation
This project focus on the acceleration of deep learning models using GPUs. Several deep learning models, including autoen- coders, recurrent neural networks (RNNs), and convolutional networks, are used for parallelization. The project examines the implementation of GPU-based training, including multi-GPU strategies using DistributedDataParal- lel (DDP). The results demonstrate the effectiveness of scaling model training across multiple GPUs, with an evaluation of performance improvements.
