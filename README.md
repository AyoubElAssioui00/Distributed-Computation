# Distributed-Computation
This project focus on the acceleration of deep learning models using GPUs. Several deep learning models, including auto-encoders, recurrent neural networks (RNNs), and convolutional networks, are used for parallelization. The project examines the implementation of GPU-based training, including multi-GPU strategies using DistributedDataParallel (DDP). The results demonstrate the effectiveness of scaling model training across multiple GPUs, with an evaluation of performance improvements.
